{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abb29cbb",
      "metadata": {
        "id": "abb29cbb"
      },
      "source": [
        "# Lab on Multi-Task Learning and Transfer Learning\n",
        "## Introduction\n",
        "In this lab, we will explore the concepts of Multi-Task Learning (MTL) and Transfer Learning. These are powerful techniques in machine learning used to leverage shared knowledge across tasks or domains. This lab is structured to provide a clear understanding of these concepts through explanations, coding exercises, and practical tasks using real-world datasets.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caee0177",
      "metadata": {
        "id": "caee0177"
      },
      "source": [
        "## Part 1: Multi-Task Learning\n",
        "### Theory\n",
        "Multi-Task Learning is a paradigm where a single model is trained on multiple related tasks simultaneously. It leverages shared representations to improve performance across all tasks. This approach is particularly beneficial when labeled data for individual tasks is limited or when tasks share underlying structures.\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation\n",
        "UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc.\n",
        "\n",
        "Link of the dataset: https://www.kaggle.com/datasets/jangedoo/utkface-new/data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading dataset from kaggle"
      ],
      "metadata": {
        "id": "vAvigOpgK7O9"
      },
      "id": "vAvigOpgK7O9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload kaggle.json file\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Move kaggle.json to the correct location and set permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the UTKFace dataset\n",
        "!kaggle datasets download -d jangedoo/utkface-new\n",
        "\n",
        "# Extract the dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('utkface-new.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('UTKFace')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "P7LvIjMe7cdZ",
        "outputId": "4cff9c4b-2246-44bb-a1a5-46b133e4c403"
      },
      "id": "P7LvIjMe7cdZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83c7023c-ac4c-4d22-92c7-ec621029ff84\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83c7023c-ac4c-4d22-92c7-ec621029ff84\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/jangedoo/utkface-new\n",
            "License(s): copyright-authors\n",
            "Downloading utkface-new.zip to /content\n",
            "100% 331M/331M [00:15<00:00, 25.1MB/s]\n",
            "100% 331M/331M [00:15<00:00, 22.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing"
      ],
      "metadata": {
        "id": "5P9J0As-LZ-X"
      },
      "id": "5P9J0As-LZ-X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b77e54",
      "metadata": {
        "id": "18b77e54"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Constants and Dataset Paths"
      ],
      "metadata": {
        "id": "vF2qwPBfLhTI"
      },
      "id": "vF2qwPBfLhTI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "IMG_SIZE = 200  # The target size to which all images will be resized (200x200)\n",
        "BATCH_SIZE = 32  # Number of images processed in each batch\n",
        "EPOCHS = 20  # Number of training epochs\n",
        "IMG_PATH = \"/content/UTKFace/UTKFace\"  # Path to the dataset on Kaggle\n"
      ],
      "metadata": {
        "id": "2RBydhHGLk8f"
      },
      "id": "2RBydhHGLk8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset (Filenames and Labels)"
      ],
      "metadata": {
        "id": "iVLpnnhSLpN7"
      },
      "id": "iVLpnnhSLpN7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (we're keeping just filenames and labels)\n",
        "image_filenames = []  # List to store image filenames\n",
        "ages = []  # List to store age labels\n",
        "genders = []  # List to store gender labels\n",
        "\n",
        "# Load image filenames and labels without loading images into memory yet\n",
        "for img_name in os.listdir(IMG_PATH):  # Loop through all the image files in the dataset\n",
        "    try:\n",
        "        # Extract age and gender from filename (e.g., '25_0_0_0_0_0_0.jpg' => age=25, gender=0)\n",
        "        age = int(img_name.split(\"_\")[0])  # Age is the first value in the filename\n",
        "        gender = int(img_name.split(\"_\")[1])  # Gender is the second value in the filename\n",
        "\n",
        "        # Append to lists\n",
        "        image_filenames.append(img_name)\n",
        "        ages.append(age)\n",
        "        genders.append(gender)\n",
        "    except Exception as e:  # Handle any errors (e.g., if the filename doesn't follow the expected pattern)\n",
        "        print(f\"Error processing {img_name}: {e}\")\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "ages = np.array(ages, dtype=np.int64)  # Convert age list to numpy array\n",
        "genders = np.array(genders, dtype=np.uint8)  # Convert gender list to numpy array\n"
      ],
      "metadata": {
        "id": "3Z2zcdaXLoCP"
      },
      "id": "3Z2zcdaXLoCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split Data into Training and Validation Sets"
      ],
      "metadata": {
        "id": "z8imEIJbLuQl"
      },
      "id": "z8imEIJbLuQl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation sets\n",
        "train_filenames, val_filenames, y_age_train, y_age_val, y_gender_train, y_gender_val = train_test_split(\n",
        "    image_filenames, ages, genders, test_size=0.2, random_state=42  # Split 80% for training and 20% for validation\n",
        ")"
      ],
      "metadata": {
        "id": "aOsidnxlLyE0"
      },
      "id": "aOsidnxlLyE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Data Generator for Loading Data in Batches"
      ],
      "metadata": {
        "id": "UztYvYrlL0Nm"
      },
      "id": "UztYvYrlL0Nm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data generator\n",
        "def data_generator(filenames, labels_age, labels_gender, batch_size):\n",
        "    while True:  # Infinite loop to keep yielding batches\n",
        "        # Create batches\n",
        "        for i in range(0, len(filenames), batch_size):  # Iterate over filenames in steps of batch_size\n",
        "            batch_filenames = filenames[i:i+batch_size]  # Get the current batch of filenames\n",
        "            batch_y_age = labels_age[i:i+batch_size]  # Get the corresponding age labels for the batch\n",
        "            batch_y_gender = labels_gender[i:i+batch_size]  # Get the corresponding gender labels for the batch\n",
        "\n",
        "            batch_images = []  # Initialize a list to store images for the current batch\n",
        "            for filename in batch_filenames:\n",
        "                img = cv2.imread(os.path.join(IMG_PATH, filename))  # Read the image from disk\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB format\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize the image to IMG_SIZE x IMG_SIZE\n",
        "                batch_images.append(img)  # Add the processed image to the batch\n",
        "\n",
        "            batch_images = np.array(batch_images, dtype=np.float32) / 255.0  # Normalize image pixel values to [0, 1]\n",
        "            yield batch_images, {'age_output': batch_y_age, 'gender_output': batch_y_gender}  # Yield images and labels as a tuple\n"
      ],
      "metadata": {
        "id": "hMCa7oiKL4bu"
      },
      "id": "hMCa7oiKL4bu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Definition"
      ],
      "metadata": {
        "id": "a7m2LA9PL7K2"
      },
      "id": "a7m2LA9PL7K2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "input_layer = Input(shape=(IMG_SIZE, IMG_SIZE, 3))  # Input layer expects images of shape (200, 200, 3)\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_layer)  # 2D convolution with 32 filters and ReLU activation\n",
        "x = MaxPooling2D((2, 2))(x)  # Max pooling with a 2x2 window\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)  # 2D convolution with 64 filters\n",
        "x = MaxPooling2D((2, 2))(x)  # Max pooling\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)  # 2D convolution with 128 filters\n",
        "x = MaxPooling2D((2, 2))(x)  # Max pooling\n",
        "x = Flatten()(x)  # Flatten the output of the convolutions into a 1D vector\n",
        "x = Dropout(0.5)(x)  # Dropout layer to reduce overfitting"
      ],
      "metadata": {
        "id": "qnhEITxML-Cp"
      },
      "id": "qnhEITxML-Cp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age prediction branch\n",
        "age_output = Dense(1, name='age_output')(x)  # Dense layer for age prediction (regression output)"
      ],
      "metadata": {
        "id": "8G83npT0MBYX"
      },
      "id": "8G83npT0MBYX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gender prediction branch\n",
        "gender_output = Dense(1, activation='sigmoid', name='gender_output')(x)  # Dense layer for gender prediction (binary classification)"
      ],
      "metadata": {
        "id": "IzgLPv55MC7L"
      },
      "id": "IzgLPv55MC7L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined model\n",
        "model = Model(inputs=input_layer, outputs=[age_output, gender_output])  # The model has two outputs: age and gender"
      ],
      "metadata": {
        "id": "3-TPGIDXMEvm"
      },
      "id": "3-TPGIDXMEvm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile and Train the Model\n"
      ],
      "metadata": {
        "id": "kmGwApU6MGJ0"
      },
      "id": "kmGwApU6MGJ0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',  # Adam optimizer for training\n",
        "    loss={'age_output': 'mean_squared_error', 'gender_output': 'binary_crossentropy'},  # Different loss functions for each output\n",
        "    metrics={'age_output': 'mae', 'gender_output': 'accuracy'}  # Metrics for evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "LRxm0YKsMKeq"
      },
      "id": "LRxm0YKsMKeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the custom generator\n",
        "history = model.fit(\n",
        "    data_generator(train_filenames, y_age_train, y_gender_train, BATCH_SIZE),  # Training data generator\n",
        "    validation_data=data_generator(val_filenames, y_age_val, y_gender_val, BATCH_SIZE),  # Validation data generator\n",
        "    steps_per_epoch=len(train_filenames) // BATCH_SIZE,  # Number of steps per epoch\n",
        "    validation_steps=len(val_filenames) // BATCH_SIZE,  # Number of validation steps\n",
        "    epochs=EPOCHS  # Number of epochs\n",
        ")\n"
      ],
      "metadata": {
        "id": "wf_eVs5FMNek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33bc63a-5d18-49ff-ffd1-b12e70d30887"
      },
      "id": "wf_eVs5FMNek",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 49ms/step - age_output_loss: 459.4293 - age_output_mae: 16.2627 - gender_output_accuracy: 0.6525 - gender_output_loss: 0.6562 - loss: 460.0854 - val_age_output_loss: 248.8406 - val_age_output_mae: 11.6558 - val_gender_output_accuracy: 0.8157 - val_gender_output_loss: 0.4178 - val_loss: 249.2584\n",
            "Epoch 2/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 45ms/step - age_output_loss: 230.1503 - age_output_mae: 11.6601 - gender_output_accuracy: 0.7884 - gender_output_loss: 0.5402 - loss: 230.8545 - val_age_output_loss: 184.4683 - val_age_output_mae: 10.6749 - val_gender_output_accuracy: 0.8435 - val_gender_output_loss: 0.4090 - val_loss: 185.3422\n",
            "Epoch 3/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - age_output_loss: 180.3463 - age_output_mae: 10.2845 - gender_output_accuracy: 0.8274 - gender_output_loss: 0.4483 - loss: 180.9346 - val_age_output_loss: 162.5773 - val_age_output_mae: 10.0236 - val_gender_output_accuracy: 0.8601 - val_gender_output_loss: 0.3843 - val_loss: 163.2944\n",
            "Epoch 4/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - age_output_loss: 160.6739 - age_output_mae: 9.7150 - gender_output_accuracy: 0.8423 - gender_output_loss: 0.4158 - loss: 161.3104 - val_age_output_loss: 134.3069 - val_age_output_mae: 8.8887 - val_gender_output_accuracy: 0.8170 - val_gender_output_loss: 0.5280 - val_loss: 135.0134\n",
            "Epoch 5/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 143.6192 - age_output_mae: 9.1961 - gender_output_accuracy: 0.8453 - gender_output_loss: 0.4096 - loss: 144.1235 - val_age_output_loss: 124.7548 - val_age_output_mae: 8.5591 - val_gender_output_accuracy: 0.8195 - val_gender_output_loss: 0.5466 - val_loss: 125.4533\n",
            "Epoch 6/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 133.1643 - age_output_mae: 8.8637 - gender_output_accuracy: 0.8554 - gender_output_loss: 0.3939 - loss: 133.6851 - val_age_output_loss: 116.2635 - val_age_output_mae: 8.1924 - val_gender_output_accuracy: 0.8624 - val_gender_output_loss: 0.4005 - val_loss: 116.5533\n",
            "Epoch 7/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - age_output_loss: 124.3803 - age_output_mae: 8.5488 - gender_output_accuracy: 0.8690 - gender_output_loss: 0.3543 - loss: 124.7310 - val_age_output_loss: 110.2047 - val_age_output_mae: 7.9879 - val_gender_output_accuracy: 0.8756 - val_gender_output_loss: 0.3471 - val_loss: 110.3814\n",
            "Epoch 8/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - age_output_loss: 116.1137 - age_output_mae: 8.2728 - gender_output_accuracy: 0.8702 - gender_output_loss: 0.3656 - loss: 116.5208 - val_age_output_loss: 104.4516 - val_age_output_mae: 7.6884 - val_gender_output_accuracy: 0.8771 - val_gender_output_loss: 0.3685 - val_loss: 104.7168\n",
            "Epoch 9/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - age_output_loss: 107.0332 - age_output_mae: 7.9410 - gender_output_accuracy: 0.8764 - gender_output_loss: 0.3631 - loss: 107.4352 - val_age_output_loss: 103.9042 - val_age_output_mae: 7.5985 - val_gender_output_accuracy: 0.8692 - val_gender_output_loss: 0.4242 - val_loss: 104.0569\n",
            "Epoch 10/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 43ms/step - age_output_loss: 102.1696 - age_output_mae: 7.7473 - gender_output_accuracy: 0.8678 - gender_output_loss: 0.4255 - loss: 102.6299 - val_age_output_loss: 102.7894 - val_age_output_mae: 7.5658 - val_gender_output_accuracy: 0.8822 - val_gender_output_loss: 0.3963 - val_loss: 102.7598\n",
            "Epoch 11/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 98.7627 - age_output_mae: 7.5890 - gender_output_accuracy: 0.8885 - gender_output_loss: 0.3526 - loss: 99.1691 - val_age_output_loss: 105.0203 - val_age_output_mae: 7.6523 - val_gender_output_accuracy: 0.8870 - val_gender_output_loss: 0.3898 - val_loss: 104.9886\n",
            "Epoch 12/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 95.4355 - age_output_mae: 7.5108 - gender_output_accuracy: 0.8925 - gender_output_loss: 0.3206 - loss: 95.8553 - val_age_output_loss: 102.6842 - val_age_output_mae: 7.7636 - val_gender_output_accuracy: 0.8790 - val_gender_output_loss: 0.4194 - val_loss: 102.8602\n",
            "Epoch 13/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 45ms/step - age_output_loss: 92.5278 - age_output_mae: 7.3955 - gender_output_accuracy: 0.8783 - gender_output_loss: 0.3980 - loss: 92.9403 - val_age_output_loss: 99.0854 - val_age_output_mae: 7.5029 - val_gender_output_accuracy: 0.8856 - val_gender_output_loss: 0.3907 - val_loss: 99.2933\n",
            "Epoch 14/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - age_output_loss: 89.3965 - age_output_mae: 7.2819 - gender_output_accuracy: 0.8895 - gender_output_loss: 0.3566 - loss: 89.7835 - val_age_output_loss: 97.4194 - val_age_output_mae: 7.4833 - val_gender_output_accuracy: 0.8439 - val_gender_output_loss: 0.6683 - val_loss: 97.9366\n",
            "Epoch 15/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - age_output_loss: 84.9729 - age_output_mae: 7.1148 - gender_output_accuracy: 0.8998 - gender_output_loss: 0.3112 - loss: 85.3212 - val_age_output_loss: 94.5551 - val_age_output_mae: 7.3515 - val_gender_output_accuracy: 0.8611 - val_gender_output_loss: 0.5228 - val_loss: 94.8901\n",
            "Epoch 16/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 83.2876 - age_output_mae: 6.9993 - gender_output_accuracy: 0.9107 - gender_output_loss: 0.2692 - loss: 83.5695 - val_age_output_loss: 94.3516 - val_age_output_mae: 7.3167 - val_gender_output_accuracy: 0.8909 - val_gender_output_loss: 0.3877 - val_loss: 94.4335\n",
            "Epoch 17/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 43ms/step - age_output_loss: 80.5508 - age_output_mae: 6.9011 - gender_output_accuracy: 0.9170 - gender_output_loss: 0.2451 - loss: 80.8490 - val_age_output_loss: 92.4755 - val_age_output_mae: 7.2698 - val_gender_output_accuracy: 0.8856 - val_gender_output_loss: 0.3869 - val_loss: 92.6546\n",
            "Epoch 18/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 77.0737 - age_output_mae: 6.7859 - gender_output_accuracy: 0.9074 - gender_output_loss: 0.2907 - loss: 77.3741 - val_age_output_loss: 95.4538 - val_age_output_mae: 7.2637 - val_gender_output_accuracy: 0.8892 - val_gender_output_loss: 0.3839 - val_loss: 95.6483\n",
            "Epoch 19/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - age_output_loss: 76.8000 - age_output_mae: 6.7482 - gender_output_accuracy: 0.9203 - gender_output_loss: 0.2287 - loss: 77.0580 - val_age_output_loss: 94.9116 - val_age_output_mae: 7.1773 - val_gender_output_accuracy: 0.8898 - val_gender_output_loss: 0.3748 - val_loss: 95.1251\n",
            "Epoch 20/20\n",
            "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - age_output_loss: 75.9249 - age_output_mae: 6.7493 - gender_output_accuracy: 0.9214 - gender_output_loss: 0.2176 - loss: 76.1679 - val_age_output_loss: 99.8230 - val_age_output_mae: 7.3284 - val_gender_output_accuracy: 0.8807 - val_gender_output_loss: 0.4113 - val_loss: 100.0105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the Model"
      ],
      "metadata": {
        "id": "ShO7O5FqMQQj"
      },
      "id": "ShO7O5FqMQQj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = model.evaluate(data_generator(val_filenames, y_age_val, y_gender_val, BATCH_SIZE),\n",
        "                         steps=len(val_filenames) // BATCH_SIZE)  # Evaluate on the validation set\n",
        "\n",
        "print(f\"Validation Age MAE: {results[1]}\")  # Index 1 for 'age_output' MAE (mean absolute error)\n",
        "print(f\"Validation Gender Accuracy: {results[2]}\")  # Index 2 for 'gender_output' accuracy\n"
      ],
      "metadata": {
        "id": "ca4087D_MTBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb75911e-bf62-4117-d3af-04ee605552fb"
      },
      "id": "ca4087D_MTBM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - age_output_loss: 100.2454 - age_output_mae: 7.3410 - gender_output_accuracy: 0.8823 - gender_output_loss: 0.4218 - loss: 100.6672\n",
            "Validation Age MAE: 99.45651245117188\n",
            "Validation Gender Accuracy: 0.4134889543056488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5358e8",
      "metadata": {
        "id": "2d5358e8"
      },
      "source": [
        "## Part 2: Transfer Learning\n",
        "### Theory\n",
        "Transfer Learning involves using a pre-trained model on a related task and fine-tuning it for a specific new task. This is especially useful when labeled data for the new task is limited.\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation\n",
        "We will use a pre-trained MobileNet model\n",
        "\n",
        "Link of Dataset: https://www.kaggle.com/datasets/nunenuh/pytorch-challange-flower-dataset/data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading dataset from kaggle"
      ],
      "metadata": {
        "id": "osMZUCixK2MA"
      },
      "id": "osMZUCixK2MA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload kaggle.json file (manually upload via the Colab interface)\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Move kaggle.json to the correct location and set permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the UTKFace dataset\n",
        "# Download the dataset using Kaggle API\n",
        "!kaggle datasets download -d nunenuh/pytorch-challange-flower-dataset\n",
        "\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip /content/pytorch-challange-flower-dataset.zip -d /content/flower_dataset/\n"
      ],
      "metadata": {
        "id": "eyPMT-xKK4oJ"
      },
      "id": "eyPMT-xKK4oJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing"
      ],
      "metadata": {
        "id": "t3uAMX5GI328"
      },
      "id": "t3uAMX5GI328"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "5vRq-0igHi_B"
      },
      "id": "5vRq-0igHi_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Constants and Dataset Paths"
      ],
      "metadata": {
        "id": "ga0Mk-dkI1SY"
      },
      "id": "ga0Mk-dkI1SY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "NUM_CLASSES = 102  # Number of flower species\n",
        "DATASET_PATH = '/content/flower_dataset/dataset'  # Replace with the actual path"
      ],
      "metadata": {
        "id": "sc5bmAN5Hk6Q"
      },
      "id": "sc5bmAN5Hk6Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset paths\n",
        "train_dir = os.path.join(DATASET_PATH, 'train')  # Path to the training set\n",
        "valid_dir = os.path.join(DATASET_PATH, 'valid') # Path to the validation set"
      ],
      "metadata": {
        "id": "BVojCq33Hm8C"
      },
      "id": "BVojCq33Hm8C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Data Generator for Loading Data in Batches"
      ],
      "metadata": {
        "id": "2XJeVbEtIwtg"
      },
      "id": "2XJeVbEtIwtg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data generator for loading data in batches\n",
        "def data_generator(filenames, labels, batch_size, dataset_path, img_size=IMG_SIZE):\n",
        "    while True:\n",
        "        # Loop through the dataset in batches\n",
        "        for i in range(0, len(filenames), batch_size):\n",
        "            batch_filenames = filenames[i:i+batch_size]  # Get the current batch of filenames\n",
        "            batch_labels = labels[i:i+batch_size]  # Get the corresponding labels for the batch\n",
        "\n",
        "            batch_images = []  # Initialize a list to hold the images for the current batch\n",
        "            for filename in batch_filenames:\n",
        "                img_path = os.path.join(dataset_path, filename)  # Get the full path of the image\n",
        "                img = Image.open(img_path).resize((img_size, img_size))  # Open and resize the image\n",
        "                img = np.array(img) / 255.0  # Normalize pixel values to the range [0, 1]\n",
        "                batch_images.append(img)  # Append the image to the batch_images list\n",
        "\n",
        "            batch_images = np.array(batch_images)  # Convert the list of images to a numpy array\n",
        "            yield batch_images, batch_labels  # Yield the batch of images and corresponding labels"
      ],
      "metadata": {
        "id": "wiUHgEQ8Hp4b"
      },
      "id": "wiUHgEQ8Hp4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Labels from Dataset"
      ],
      "metadata": {
        "id": "eeGCo6VGJExj"
      },
      "id": "eeGCo6VGJExj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the labels\n",
        "def load_labels(dataset_path):\n",
        "    label_dict = {}  # Initialize an empty dictionary to map folder names to label indices\n",
        "    for label_dir in os.listdir(dataset_path):  # Loop through the directories (which are labeled)\n",
        "        label_dict[label_dir] = int(label_dir) - 1  # Map folder names to labels (0-indexed)\n",
        "    return label_dict\n"
      ],
      "metadata": {
        "id": "aLNX8C6_Htl4"
      },
      "id": "aLNX8C6_Htl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Filenames and Labels for Train and Validation Data"
      ],
      "metadata": {
        "id": "DDu3zX_KJQy2"
      },
      "id": "DDu3zX_KJQy2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get filenames and labels for train and validation datasets\n",
        "def get_filenames_and_labels(dataset_path, label_dict):\n",
        "    filenames = []\n",
        "    labels = []\n",
        "    for label_dir in os.listdir(dataset_path):  # Loop through each label directory\n",
        "        label = label_dict[label_dir]  # Get the corresponding label for the folder\n",
        "        label_path = os.path.join(dataset_path, label_dir)  # Get the path to the current label folder\n",
        "        if os.path.isdir(label_path):  # Check if it is a directory\n",
        "            for filename in os.listdir(label_path):  # Loop through each file in the label directory\n",
        "                filenames.append(os.path.join(label_dir, filename))  # Append the file path to filenames\n",
        "                labels.append(label)  # Append the corresponding label\n",
        "    return filenames, np.array(labels)  # Return the filenames and their corresponding labels\n"
      ],
      "metadata": {
        "id": "Fcj09vg3HxMc"
      },
      "id": "Fcj09vg3HxMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data and Set Up the Model"
      ],
      "metadata": {
        "id": "9981QzQoJY50"
      },
      "id": "9981QzQoJY50"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the labels and filenames\n",
        "label_dict = load_labels(train_dir)  # Load labels for the training dataset\n",
        "train_filenames, y_train = get_filenames_and_labels(train_dir, label_dict)  # Get filenames and labels for training\n",
        "val_filenames, y_val = get_filenames_and_labels(valid_dir, label_dict)  # Get filenames and labels for validation\n"
      ],
      "metadata": {
        "id": "ys9UQqVhHzP7"
      },
      "id": "ys9UQqVhHzP7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Pre-trained VGG16 Model and Freeze Base Layers"
      ],
      "metadata": {
        "id": "00L0H0yiJfse"
      },
      "id": "00L0H0yiJfse"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model + higher level layers (without the top layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Set all layers in the base model to non-trainable"
      ],
      "metadata": {
        "id": "EieE30y_H1C4"
      },
      "id": "EieE30y_H1C4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build and Compile the Custom Model"
      ],
      "metadata": {
        "id": "XPt3REwLJr9P"
      },
      "id": "XPt3REwLJr9P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "x = Flatten()(base_model.output)  # Flatten the output of the base model\n",
        "x = Dropout(0.5)(x)  # Add dropout to reduce overfitting\n",
        "x = Dense(102, activation='softmax')(x)  # Add a dense layer with 102 output units (one per class) with softmax activation\n"
      ],
      "metadata": {
        "id": "-lfbosRcH4Da"
      },
      "id": "-lfbosRcH4Da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine base model with the custom layers\n",
        "model = Model(inputs=base_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "UCqcuxm-H6lJ"
      },
      "id": "UCqcuxm-H6lJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Complie Model"
      ],
      "metadata": {
        "id": "quDMmgU9Jwje"
      },
      "id": "quDMmgU9Jwje"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i-kVmCfCH-A9"
      },
      "id": "i-kVmCfCH-A9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "Q2T-T8HKJ1DP"
      },
      "id": "Q2T-T8HKJ1DP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the custom generator\n",
        "steps_per_epoch = len(train_filenames) // BATCH_SIZE  # Number of steps per epoch (how many batches per epoch)\n",
        "validation_steps = len(val_filenames) // BATCH_SIZE  # Number of validation steps (how many validation batches)"
      ],
      "metadata": {
        "id": "rQdF5zM5IAwc"
      },
      "id": "rQdF5zM5IAwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    data_generator(train_filenames, y_train, BATCH_SIZE, train_dir),\n",
        "    validation_data=data_generator(val_filenames, y_val, BATCH_SIZE, valid_dir),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9IlxYykHOce",
        "outputId": "f9061a0c-4eed-445d-b968-cd6e63ba7343"
      },
      "id": "F9IlxYykHOce",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 377ms/step - accuracy: 0.0810 - loss: 19.0212 - val_accuracy: 0.0250 - val_loss: 27.9913\n",
            "Epoch 2/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 392ms/step - accuracy: 0.0024 - loss: 26.9267 - val_accuracy: 0.0471 - val_loss: 31.5160\n",
            "Epoch 3/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 344ms/step - accuracy: 0.0400 - loss: 24.8290 - val_accuracy: 0.0611 - val_loss: 29.0145\n",
            "Epoch 4/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 343ms/step - accuracy: 0.0880 - loss: 18.5215 - val_accuracy: 0.1896 - val_loss: 15.6842\n",
            "Epoch 5/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 330ms/step - accuracy: 0.2108 - loss: 12.2000 - val_accuracy: 0.2176 - val_loss: 15.4504\n",
            "Epoch 6/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 345ms/step - accuracy: 0.3602 - loss: 8.9980 - val_accuracy: 0.2774 - val_loss: 14.0036\n",
            "Epoch 7/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 349ms/step - accuracy: 0.4432 - loss: 7.6736 - val_accuracy: 0.3588 - val_loss: 10.8671\n",
            "Epoch 8/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 342ms/step - accuracy: 0.5234 - loss: 5.4764 - val_accuracy: 0.4313 - val_loss: 8.9111\n",
            "Epoch 9/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 326ms/step - accuracy: 0.5990 - loss: 4.1369 - val_accuracy: 0.3791 - val_loss: 10.6853\n",
            "Epoch 10/10\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 352ms/step - accuracy: 0.6567 - loss: 3.4891 - val_accuracy: 0.4135 - val_loss: 9.0707\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 342ms/step - accuracy: 0.2576 - loss: 11.4786\n",
            "Validation Loss: 9.087573051452637\n",
            "Validation Accuracy: 0.4112499952316284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evalute the Model"
      ],
      "metadata": {
        "id": "7AAaH6vXJ97F"
      },
      "id": "7AAaH6vXJ97F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = model.evaluate(data_generator(val_filenames, y_val, BATCH_SIZE, valid_dir), steps=validation_steps)\n",
        "print(f\"Validation Loss: {results[0]}\")\n",
        "print(f\"Validation Accuracy: {results[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd0mzHkAHROE",
        "outputId": "2c36fb69-98ff-40a3-88cd-e3d92719352a"
      },
      "id": "bd0mzHkAHROE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 275ms/step - accuracy: 0.2576 - loss: 11.4786\n",
            "Validation Loss: 9.087573051452637\n",
            "Validation Accuracy: 0.4112499952316284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c119f2f",
      "metadata": {
        "id": "4c119f2f"
      },
      "source": [
        "\n",
        "#Assignment: Multi-Task Learning for Fashion Product Image Classification\n",
        "##Overview:\n",
        "In this assignment, you will use Multi-Task Learning (MTL) to classify fashion product images into three categories using the Fashion Product Images (Small) dataset:\n",
        "\n",
        "* Article Category Classification (e.g., T-shirt, Jeans)\n",
        "* Base Color Classification (e.g., Red, Blue)\n",
        "* Target Season Classification (e.g., Summer, Winter)\n",
        "\n",
        "**You will build a single model that predicts all three tasks simultaneously by sharing the same feature extraction layers and using separate output branches for each task.**\n",
        "\n",
        "Link to dataset: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ],
      "metadata": {
        "id": "A-mOCUcisHDE"
      },
      "id": "A-mOCUcisHDE",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!kaggle datasets download -d paramaggarwal/fashion-product-images-small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DBW7Q0jsHr8",
        "outputId": "0173dcac-bf40-40dc-953d-07309d3e2da0"
      },
      "id": "7DBW7Q0jsHr8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small\n",
            "License(s): MIT\n",
            "Downloading fashion-product-images-small.zip to /content\n",
            " 98% 553M/565M [00:03<00:00, 144MB/s]\n",
            "100% 565M/565M [00:03<00:00, 148MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(\"fashion-product-images-small.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"fashion_dataset\")"
      ],
      "metadata": {
        "id": "Z7UR10QbsT2D"
      },
      "id": "Z7UR10QbsT2D",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the dataset directories\n",
        "data_dir = \"fashion_dataset/myntradataset/images\"\n",
        "metadata_path = \"fashion_dataset/myntradataset/styles.csv\""
      ],
      "metadata": {
        "id": "du7lqt6tsdSP"
      },
      "id": "du7lqt6tsdSP",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metadata\n",
        "df= pd.read_csv(metadata_path, on_bad_lines='skip')\n",
        "df.dropna(subset=['baseColour', 'season', 'articleType'], inplace=True)"
      ],
      "metadata": {
        "id": "jAQ18ZYOsr0a"
      },
      "id": "jAQ18ZYOsr0a",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter metadata to include only rows with available images\n",
        "df['image_path'] = df['id'].astype(str) + \".jpg\"\n",
        "df = df[df['image_path'].apply(lambda x: os.path.isfile(os.path.join(data_dir, x)))]"
      ],
      "metadata": {
        "id": "Pyxsl_fksvnM"
      },
      "id": "Pyxsl_fksvnM",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "article_encoder = LabelEncoder()\n",
        "color_encoder = LabelEncoder()\n",
        "season_encoder = LabelEncoder()\n",
        "\n",
        "df['articleType'] = article_encoder.fit_transform(df['articleType'])\n",
        "df['baseColour'] = color_encoder.fit_transform(df['baseColour'])\n",
        "df['season'] = season_encoder.fit_transform(df['season'])\n"
      ],
      "metadata": {
        "id": "NknQW2IKs0Qh"
      },
      "id": "NknQW2IKs0Qh",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define paths for train and validation image sets\n",
        "train_image_paths = train_df['image_path'].apply(lambda x: os.path.join(data_dir, x)).tolist()\n",
        "valid_image_paths = valid_df['image_path'].apply(lambda x: os.path.join(data_dir, x)).tolist()\n",
        "\n",
        "train_labels = train_df[['articleType', 'baseColour', 'season']].values\n",
        "valid_labels = valid_df[['articleType', 'baseColour', 'season']].values"
      ],
      "metadata": {
        "id": "ZuVxqHaCs3Ud"
      },
      "id": "ZuVxqHaCs3Ud",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom data generator\n",
        "class MultiTaskDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_samples = len(image_paths)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.num_samples / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_image_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        images = []\n",
        "        article_labels = []\n",
        "        color_labels = []\n",
        "        season_labels = []\n",
        "\n",
        "        for i, path in enumerate(batch_image_paths):\n",
        "            img = load_img(path, target_size=(128, 128))\n",
        "            img = img_to_array(img) / 255.0\n",
        "            images.append(img)\n",
        "\n",
        "            article_labels.append(batch_labels[i][0])\n",
        "            color_labels.append(batch_labels[i][1])\n",
        "            season_labels.append(batch_labels[i][2])\n",
        "\n",
        "        return (\n",
        "            np.array(images),\n",
        "            {\n",
        "                'article_output': np.array(article_labels),\n",
        "                'color_output': np.array(color_labels),\n",
        "                'season_output': np.array(season_labels),\n",
        "            }\n",
        "        )"
      ],
      "metadata": {
        "id": "Nj-XpyoZs6tY"
      },
      "id": "Nj-XpyoZs6tY",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and validation generators\n",
        "batch_size = 32\n",
        "train_generator = MultiTaskDataGenerator(train_image_paths, train_labels, batch_size)\n",
        "valid_generator = MultiTaskDataGenerator(valid_image_paths, valid_labels, batch_size)"
      ],
      "metadata": {
        "id": "Sbc40P-exXqz"
      },
      "id": "Sbc40P-exXqz",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "input_layer = Input(shape=(128, 128, 3))"
      ],
      "metadata": {
        "id": "s61esaDyxbjS"
      },
      "id": "s61esaDyxbjS",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shared feature extractor\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "x = layers.Flatten()(x)"
      ],
      "metadata": {
        "id": "_PNYYszSs9xk"
      },
      "id": "_PNYYszSs9xk",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output branches\n",
        "article_output = layers.Dense(len(article_encoder.classes_), activation='softmax', name='article_output')(x)\n",
        "color_output = layers.Dense(len(color_encoder.classes_), activation='softmax', name='color_output')(x)\n",
        "season_output = layers.Dense(len(season_encoder.classes_), activation='softmax', name='season_output')(x)"
      ],
      "metadata": {
        "id": "KhYjQ-MItDbD"
      },
      "id": "KhYjQ-MItDbD",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = models.Model(inputs=input_layer, outputs=[article_output, color_output, season_output])"
      ],
      "metadata": {
        "id": "vcufI-eBtI-Y"
      },
      "id": "vcufI-eBtI-Y",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'article_output': 'sparse_categorical_crossentropy',\n",
        "        'color_output': 'sparse_categorical_crossentropy',\n",
        "        'season_output': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'article_output': 'accuracy',\n",
        "        'color_output': 'accuracy',\n",
        "        'season_output': 'accuracy',\n",
        "    }  # Specify metrics for each output by name\n",
        ")"
      ],
      "metadata": {
        "id": "wQs8lXaPtLmM"
      },
      "id": "wQs8lXaPtLmM",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=valid_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_steps=len(valid_generator)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8-tNdYutN-S",
        "outputId": "76a839d5-0622-4570-b0e9-364a69d3f902"
      },
      "id": "s8-tNdYutN-S",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 47ms/step - article_output_accuracy: 0.5779 - article_output_loss: 1.8451 - color_output_accuracy: 0.4545 - color_output_loss: 2.0376 - loss: 4.8798 - season_output_accuracy: 0.5883 - season_output_loss: 0.9973 - val_article_output_accuracy: 0.7806 - val_article_output_loss: 0.8175 - val_color_output_accuracy: 0.5916 - val_color_output_loss: 1.4532 - val_loss: 3.0727 - val_season_output_accuracy: 0.6631 - val_season_output_loss: 0.8005\n",
            "Epoch 2/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24us/step - article_output_accuracy: 0.0000e+00 - article_output_loss: 0.0000e+00 - color_output_accuracy: 0.0000e+00 - color_output_loss: 0.0000e+00 - loss: 0.0000e+00 - season_output_accuracy: 0.0000e+00 - season_output_loss: 0.0000e+00\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 30ms/step - article_output_accuracy: 0.8320 - article_output_loss: 0.5735 - color_output_accuracy: 0.6294 - color_output_loss: 1.2340 - loss: 2.5350 - season_output_accuracy: 0.6972 - season_output_loss: 0.7278 - val_article_output_accuracy: 0.8033 - val_article_output_loss: 0.7081 - val_color_output_accuracy: 0.5946 - val_color_output_loss: 1.4509 - val_loss: 2.8886 - val_season_output_accuracy: 0.7052 - val_season_output_loss: 0.7286\n",
            "Epoch 4/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273us/step - article_output_accuracy: 0.0000e+00 - article_output_loss: 0.0000e+00 - color_output_accuracy: 0.0000e+00 - color_output_loss: 0.0000e+00 - loss: 0.0000e+00 - season_output_accuracy: 0.0000e+00 - season_output_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 30ms/step - article_output_accuracy: 0.8994 - article_output_loss: 0.3195 - color_output_accuracy: 0.7041 - color_output_loss: 0.9573 - loss: 1.9207 - season_output_accuracy: 0.7424 - season_output_loss: 0.6439 - val_article_output_accuracy: 0.8116 - val_article_output_loss: 0.7154 - val_color_output_accuracy: 0.6090 - val_color_output_loss: 1.4979 - val_loss: 2.9395 - val_season_output_accuracy: 0.7131 - val_season_output_loss: 0.7248\n",
            "Epoch 6/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15us/step - article_output_accuracy: 0.0000e+00 - article_output_loss: 0.0000e+00 - color_output_accuracy: 0.0000e+00 - color_output_loss: 0.0000e+00 - loss: 0.0000e+00 - season_output_accuracy: 0.0000e+00 - season_output_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 29ms/step - article_output_accuracy: 0.9408 - article_output_loss: 0.1855 - color_output_accuracy: 0.7694 - color_output_loss: 0.7322 - loss: 1.4867 - season_output_accuracy: 0.7720 - season_output_loss: 0.5689 - val_article_output_accuracy: 0.8148 - val_article_output_loss: 0.7536 - val_color_output_accuracy: 0.6004 - val_color_output_loss: 1.5984 - val_loss: 3.1087 - val_season_output_accuracy: 0.7107 - val_season_output_loss: 0.7551\n",
            "Epoch 8/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15us/step - article_output_accuracy: 0.0000e+00 - article_output_loss: 0.0000e+00 - color_output_accuracy: 0.0000e+00 - color_output_loss: 0.0000e+00 - loss: 0.0000e+00 - season_output_accuracy: 0.0000e+00 - season_output_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 30ms/step - article_output_accuracy: 0.9665 - article_output_loss: 0.1071 - color_output_accuracy: 0.8236 - color_output_loss: 0.5538 - loss: 1.1832 - season_output_accuracy: 0.7903 - season_output_loss: 0.5223 - val_article_output_accuracy: 0.8211 - val_article_output_loss: 0.8400 - val_color_output_accuracy: 0.5964 - val_color_output_loss: 1.7574 - val_loss: 3.3826 - val_season_output_accuracy: 0.7100 - val_season_output_loss: 0.7841\n",
            "Epoch 10/10\n",
            "\u001b[1m1110/1110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15us/step - article_output_accuracy: 0.0000e+00 - article_output_loss: 0.0000e+00 - color_output_accuracy: 0.0000e+00 - color_output_loss: 0.0000e+00 - loss: 0.0000e+00 - season_output_accuracy: 0.0000e+00 - season_output_loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('multi_task_fashion_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9C3bF7XtQmi",
        "outputId": "c68d1caf-e106-41af-99c6-f0665884a252"
      },
      "id": "G9C3bF7XtQmi",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = model.evaluate(valid_generator)\n",
        "print(f\"Test Loss and Accuracy: {results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9b1acnJyDtM",
        "outputId": "67cad595-b642-453b-93c0-ccb79059b681"
      },
      "id": "b9b1acnJyDtM",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - article_output_accuracy: 0.8162 - article_output_loss: 0.8354 - color_output_accuracy: 0.6017 - color_output_loss: 1.7153 - loss: 3.3269 - season_output_accuracy: 0.7094 - season_output_loss: 0.7762\n",
            "Test Loss and Accuracy: [3.3826420307159424, 0.8399555683135986, 1.7574396133422852, 0.7840583324432373, 0.821110725402832, 0.596372663974762, 0.7100371718406677]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-WvDzQWyIPH"
      },
      "id": "s-WvDzQWyIPH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}